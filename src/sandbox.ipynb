{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 20:59:43.146006: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from helper import GenomeCreator\n",
    "\n",
    "gc = GenomeCreator()\n",
    "path_to_daily_csv = \"~/class/f24/cbm/evo-life/prepped/daily.csv\"\n",
    "gc.load_data_from_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "genomes = gc.create_all_genomes() # dictionary of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genomes represented: \n",
    "gc.write_genomes_to_csv(path='~/class/f24/cbm/evo-life/prepped/encodings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>day</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>621e32af67b776a24045b4cf</td>\n",
       "      <td>0</td>\n",
       "      <td>[ 0  5  4  8  7  0  0  0 11  6  9  2  3 10  0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>621e32af67b776a24045b4cf</td>\n",
       "      <td>1</td>\n",
       "      <td>[ 1  5  4 10  8  0  0  0 14  6 11  3  2 13 15 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>621e32af67b776a24045b4cf</td>\n",
       "      <td>2</td>\n",
       "      <td>[ 2  4  3  9  7  0  0  0 13  5 10  0  0 12 14 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>621e32af67b776a24045b4cf</td>\n",
       "      <td>3</td>\n",
       "      <td>[ 2  6  5 11  9  0  0  0 15  7 12  4  3 14 16 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>621e32af67b776a24045b4cf</td>\n",
       "      <td>4</td>\n",
       "      <td>[2 0 0 0 0 0 0 0 8 5 6 3 4 7 0 0 0 0 1 0 1 0 1 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>0</td>\n",
       "      <td>621e30b267b776a240c5e13f</td>\n",
       "      <td>63</td>\n",
       "      <td>[1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>0</td>\n",
       "      <td>621e30b267b776a240c5e13f</td>\n",
       "      <td>64</td>\n",
       "      <td>[1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>0</td>\n",
       "      <td>621e30b267b776a240c5e13f</td>\n",
       "      <td>65</td>\n",
       "      <td>[1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0</td>\n",
       "      <td>621e30b267b776a240c5e13f</td>\n",
       "      <td>66</td>\n",
       "      <td>[1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>0</td>\n",
       "      <td>621e30b267b776a240c5e13f</td>\n",
       "      <td>67</td>\n",
       "      <td>[1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1268 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                        id  day  \\\n",
       "0              0  621e32af67b776a24045b4cf    0   \n",
       "1              0  621e32af67b776a24045b4cf    1   \n",
       "2              0  621e32af67b776a24045b4cf    2   \n",
       "3              0  621e32af67b776a24045b4cf    3   \n",
       "4              0  621e32af67b776a24045b4cf    4   \n",
       "...          ...                       ...  ...   \n",
       "1263           0  621e30b267b776a240c5e13f   63   \n",
       "1264           0  621e30b267b776a240c5e13f   64   \n",
       "1265           0  621e30b267b776a240c5e13f   65   \n",
       "1266           0  621e30b267b776a240c5e13f   66   \n",
       "1267           0  621e30b267b776a240c5e13f   67   \n",
       "\n",
       "                                               encoding  \n",
       "0     [ 0  5  4  8  7  0  0  0 11  6  9  2  3 10  0 ...  \n",
       "1     [ 1  5  4 10  8  0  0  0 14  6 11  3  2 13 15 ...  \n",
       "2     [ 2  4  3  9  7  0  0  0 13  5 10  0  0 12 14 ...  \n",
       "3     [ 2  6  5 11  9  0  0  0 15  7 12  4  3 14 16 ...  \n",
       "4     [2 0 0 0 0 0 0 0 8 5 6 3 4 7 0 0 0 0 1 0 1 0 1 0]  \n",
       "...                                                 ...  \n",
       "1263  [1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]  \n",
       "1264  [1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]  \n",
       "1265  [1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]  \n",
       "1266  [1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]  \n",
       "1267  [1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]  \n",
       "\n",
       "[1268 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('~/class/f24/cbm/evo-life/prepped/encodings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
=======
>>>>>>> 56eb08bdfa94219fca82f1a3f16bdffa2933179d
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "{'621e32af67b776a24045b4cf': <models.Genome object at 0x137a59410>, '621e337667b776a240ce78ab': <models.Genome object at 0x13abc0190>, '621e33b067b776a240f39e56': <models.Genome object at 0x13034ae10>, '621e375b67b776a240290cdc': <models.Genome object at 0x12d0e2510>, '621e2e8e67b776a24055b564': <models.Genome object at 0x13aec7d10>, '621e2fce67b776a240279baa': <models.Genome object at 0x123495c90>, '621e326767b776a24012e179': <models.Genome object at 0x13aec6e50>, '621e2f3967b776a240c654db': <models.Genome object at 0x13aec5950>, '621e329067b776a2402ffad2': <models.Genome object at 0x13aec5590>, '621e333967b776a240a3cd06': <models.Genome object at 0x13aee5ed0>, '621e339967b776a240e502de': <models.Genome object at 0x13aee5490>, '621e32e667b776a2406d2f1c': <models.Genome object at 0x13aee6b90>, '621e2eaf67b776a2406b14ac': <models.Genome object at 0x13aec5d90>, '621e33cf67b776a240087de9': <models.Genome object at 0x13aee4b50>, '621e366567b776a24076a727': <models.Genome object at 0x1379c8850>, '621e30b267b776a240c5e13f': <models.Genome object at 0x13aec4810>}\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0, len(genomes['621e32af67b776a24045b4cf'].geneset)):\n",
    "#     print(genomes['621e32af67b776a24045b4cf'].geneset[i])\n",
    "print(genomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[2 0 0 6 3 0 0 0 0 0 0 0 0 0 8 7 4 5 1 1 0 0 1 0]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
=======
      "id: 621e32af67b776a24045b4cf data: [np.float64(0.5210522151898734), np.float64(0.5175957207207207), np.float64(0.4821178393524284), np.float64(0.5055970149253731)]\n",
      "id: 621e337667b776a240ce78ab data: [np.float64(0.5860745614035088), np.float64(0.6317979708775749), np.float64(0.710650503252062), np.float64(0.8006677140612726)]\n",
      "id: 621e33b067b776a240f39e56 data: [np.float64(0.5392909356725146), np.float64(0.5685721953837897), np.float64(0.643847687400319), np.float64(0.7177083333333334)]\n",
      "id: 621e375b67b776a240290cdc data: [np.float64(0.7782561641257294), np.float64(0.75), np.float64(0.7234567901234568), np.float64(0.7606418918918919)]\n",
      "id: 621e2e8e67b776a24055b564 data: [np.float64(0.6288461538461538), np.float64(0.6624351624351624), np.float64(0.7387901222132689), np.float64(0.7474747474747474)]\n",
      "id: 621e2fce67b776a240279baa data: [np.float64(0.6454545454545455), np.float64(0.44805866601752675), np.float64(0.5754605263157895), np.float64(0.6202926497277677)]\n",
      "id: 621e326767b776a24012e179 data: [np.float64(0.7384046052631579), np.float64(0.7210600534373808), np.float64(0.6117474866942638), np.float64(0.6798598057644111)]\n",
      "id: 621e2f3967b776a240c654db data: [np.float64(0.6848501461988304), np.float64(0.724196556671449), np.float64(0.7166598583877996), np.float64(0.6805555555555556)]\n",
      "id: 621e329067b776a2402ffad2 data: [np.float64(0.4602287581699346), np.float64(0.5685294117647058), np.float64(0.7876750700280113), np.float64(0.6600438596491227)]\n",
      "id: 621e333967b776a240a3cd06 data: [np.float64(0.7450892857142857), np.float64(0.7282297178130512), np.float64(0.6931306306306306), np.float64(0.7771566043434095)]\n",
      "id: 621e339967b776a240e502de data: [np.float64(0.7930222602739726), np.float64(0.750975935828877), np.float64(0.7714572192513369), np.float64(0.7646296296296297)]\n",
      "id: 621e32e667b776a2406d2f1c data: [np.float64(0.7672602739726028), np.float64(0.6513310185185185), np.float64(0.7727727727727728), np.float64(0.7802206851119895)]\n",
      "id: 621e2eaf67b776a2406b14ac data: [np.float64(0.8170849420849421), np.float64(0.7645731707317074), np.float64(0.7374531835205993), np.float64(0.6563568010936432)]\n",
      "id: 621e33cf67b776a240087de9 data: [np.float64(0.8119318181818183), np.float64(0.6072887698698802), np.float64(0.5996031746031746), np.float64(0.5918181818181818)]\n",
      "id: 621e366567b776a24076a727 data: [np.float64(0.6573464912280702), np.float64(0.6637955182072829), np.float64(0.6585332817337461), np.float64(0.6114709595959595)]\n",
      "id: 621e30b267b776a240c5e13f data: [np.float64(0.7302887735236859), np.float64(0.7465097402597403), np.float64(0.7540922619047619), np.float64(0.7647799575821845)]\n"
     ]
>>>>>>> 56eb08bdfa94219fca82f1a3f16bdffa2933179d
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "daily = pd.read_csv(\"~/class/f24/cbm/evo-life/prepped/daily.csv\")\n",
    "individuals = pd.read_csv(\"~/class/f24/cbm/evo-life/prepped/individuals.csv\")\n",
    "daily['bmi'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Genome\n",
    "genome = Genome(None, None)\n",
    "phenotype_data = gc.get_all_phenotype_data()\n",
    "phenotypes = []\n",
    "for k, v in phenotype_data.items():\n",
    "    phenotypes.append(genome.calculate_genome_phenotype_split(v['anxiety'], v['tired'], v['stress_score'], v['sleep_points_percentage']))\n",
    "\n",
    "for k in range(len(phenotype_data.keys())):\n",
    "    print(f'id: {list(phenotype_data.keys())[k]} data: {phenotypes[k]}')\n",
    "\n",
    "data = {\n",
    "    \"id\": list(phenotype_data.keys()),  # Extract the IDs\n",
    "    \"month_1\": [p[0] for p in phenotypes],\n",
    "    \"month_2\": [p[1] for p in phenotypes],\n",
    "    \"month_3\": [p[2] for p in phenotypes],\n",
    "    \"month_4\": [p[3] for p in phenotypes],\n",
    "}\n",
    "\n",
    "# phenotype_df = pd.DataFrame(data)\n",
    "\n",
    "# csv_file_path = \"../prepped/phenotype_data.csv\"\n",
    "# phenotype_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# print(f\"DataFrame saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping genotype encoding to fitness value\n",
    "genotype_encodings_df = pd.read_csv(\"../prepped/encodings.csv\")\n",
    "fitness_values_df = pd.read_csv(\"../prepped/phenotype_data.csv\")\n",
    "genotype_encodings_ids = genotype_encodings_df['id'].unique()\n",
    "\n",
    "id_to_encoding = {}\n",
    "days = []\n",
    "split_size = 16\n",
    "for g_id in genotype_encodings_ids:\n",
    "    id_encodings = genotype_encodings_df[genotype_encodings_df['id'] == g_id]['encoding'].to_numpy()\n",
    "    day_split_size = int(np.floor(len(id_encodings) / 4.0))\n",
    "    id_to_encoding[g_id] = []\n",
    "    for i in range(4):\n",
    "        curr_arr = []\n",
    "        for j in range(split_size):\n",
    "            curr_arr.append(id_encodings[i*day_split_size + j])\n",
    "        curr_arr_formatted = []\n",
    "        for array_string in curr_arr:\n",
    "            array = np.fromstring(array_string.strip(\"[]\"), sep=\" \")\n",
    "            curr_arr_formatted.append(array)\n",
    "        for a in range(len(curr_arr_formatted)):\n",
    "            if curr_arr_formatted[a].shape == (26,):\n",
    "                print(i * day_split_size + a)\n",
    "                print(curr_arr_formatted[a])\n",
    "                print(g_id)\n",
    "        curr_arr_formatted = np.array(curr_arr_formatted)\n",
    "        curr_arr_formatted = curr_arr_formatted.flatten()\n",
    "        id_to_encoding[g_id].append(curr_arr_formatted)\n",
    "    id_to_encoding[g_id] = np.array(id_to_encoding[g_id])\n",
    "all_genotype_data = []\n",
    "all_fitness_data = []\n",
    "fitness_values_df['id']\n",
    "for g_id, encoding_data in id_to_encoding.items():\n",
    "    # print(f'id: {g_id} data: {(encoding_data)}')\n",
    "    all_genotype_data.append(encoding_data)\n",
    "    all_fitness_data.append(np.array([fitness_values_df[fitness_values_df['id']\n",
    " == g_id]['month_1'].to_numpy()[0], fitness_values_df[fitness_values_df['id']\n",
    " == g_id]['month_2'].to_numpy()[0], fitness_values_df[fitness_values_df['id']\n",
    " == g_id]['month_3'].to_numpy()[0], fitness_values_df[fitness_values_df['id']\n",
    " == g_id]['month_4'].to_numpy()[0]]))\n",
    "    \n",
    "all_fitness_data = np.array(all_fitness_data)\n",
    "all_genotype_data = np.array(all_genotype_data)\n",
    "\n",
    "all_p_data = []\n",
    "all_g_data = []\n",
    "\n",
    "# Flatten only the first dimension\n",
    "flattened_fitness = all_fitness_data.reshape(-1, *all_fitness_data.shape[2:])\n",
    "flattened_genotype = all_genotype_data.reshape(-1, *all_genotype_data.shape[2:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with LR=0.0001, Batch Size=8, Epochs=200\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0395\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0505\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0415\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0186\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1839\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0733\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0584\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0925\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0312\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1170\n",
      "Mean Validation Loss: 0.0706\n",
      "Training with LR=0.0001, Batch Size=8, Epochs=300\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0449\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0506\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0172\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0170\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0637\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0244\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.1006\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0944\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.1950\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0635\n",
      "Mean Validation Loss: 0.0671\n",
      "Training with LR=0.0001, Batch Size=8, Epochs=500\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.4126\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0284\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0288\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0162\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0735\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0245\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0619\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0911\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0510\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1130\n",
      "Mean Validation Loss: 0.0901\n",
      "Training with LR=0.0001, Batch Size=8, Epochs=1000\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0335\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0721\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0263\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.5105\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0359\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0164\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0931\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0760\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0243\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1173\n",
      "Mean Validation Loss: 0.1005\n",
      "Training with LR=0.0001, Batch Size=16, Epochs=200\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0277\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0436\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0173\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0816\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0424\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.3168\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0317\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.1432\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0632\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1627\n",
      "Mean Validation Loss: 0.0930\n",
      "Training with LR=0.0001, Batch Size=16, Epochs=300\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0355\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0433\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0154\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0156\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.4145\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0400\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0253\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0946\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.1844\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1405\n",
      "Mean Validation Loss: 0.1009\n",
      "Training with LR=0.0001, Batch Size=16, Epochs=500\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0263\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0532\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0199\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0258\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1253\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0284\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0220\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0586\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0679\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0947\n",
      "Mean Validation Loss: 0.0522\n",
      "Training with LR=0.0001, Batch Size=16, Epochs=1000\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0362\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0449\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0371\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.1041\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1308\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0204\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0381\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0711\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0791\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0932\n",
      "Mean Validation Loss: 0.0655\n",
      "Training with LR=0.0001, Batch Size=32, Epochs=200\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0826\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0489\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0205\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0215\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0361\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0491\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0338\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0626\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0572\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1190\n",
      "Mean Validation Loss: 0.0531\n",
      "Training with LR=0.0001, Batch Size=32, Epochs=300\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0399\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0413\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0134\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0122\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0954\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0278\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0531\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.1170\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0624\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0676\n",
      "Mean Validation Loss: 0.0530\n",
      "Training with LR=0.0001, Batch Size=32, Epochs=500\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0427\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0615\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0267\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.2695\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1334\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0099\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.1413\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0799\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0619\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0873\n",
      "Mean Validation Loss: 0.0914\n",
      "Training with LR=0.0001, Batch Size=32, Epochs=1000\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0249\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0625\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0290\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0695\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.3916\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0270\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0195\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0616\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0301\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1159\n",
      "Mean Validation Loss: 0.0832\n",
      "Training with LR=0.001, Batch Size=8, Epochs=200\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0508\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0630\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0155\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0526\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0253\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0238\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0284\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.1006\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0549\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0653\n",
      "Mean Validation Loss: 0.0480\n",
      "Training with LR=0.001, Batch Size=8, Epochs=300\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0297\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0523\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0139\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0200\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0849\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0230\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0628\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0688\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0839\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0999\n",
      "Mean Validation Loss: 0.0539\n",
      "Training with LR=0.001, Batch Size=8, Epochs=500\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.2926\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0481\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0413\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0317\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0444\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0166\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0587\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0924\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0553\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0890\n",
      "Mean Validation Loss: 0.0770\n",
      "Training with LR=0.001, Batch Size=8, Epochs=1000\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0260\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0175\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0632\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0202\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1233\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.3967\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0303\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.1013\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0442\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0843\n",
      "Mean Validation Loss: 0.0907\n",
      "Training with LR=0.001, Batch Size=16, Epochs=200\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0429\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0416\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0255\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0914\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1623\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0201\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0267\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0990\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0398\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0682\n",
      "Mean Validation Loss: 0.0618\n",
      "Training with LR=0.001, Batch Size=16, Epochs=300\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0685\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0587\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0153\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.5601\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0660\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0304\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0431\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0810\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0355\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0716\n",
      "Mean Validation Loss: 0.1030\n",
      "Training with LR=0.001, Batch Size=16, Epochs=500\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.1270\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0482\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0140\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0793\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0364\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0238\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0294\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0808\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.1377\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1069\n",
      "Mean Validation Loss: 0.0683\n",
      "Training with LR=0.001, Batch Size=16, Epochs=1000\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0571\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.3525\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0159\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0529\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0256\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0460\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0278\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0705\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0262\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.2057\n",
      "Mean Validation Loss: 0.0880\n",
      "Training with LR=0.001, Batch Size=32, Epochs=200\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0185\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0594\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0154\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0474\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0524\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.2680\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0309\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0825\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0272\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0611\n",
      "Mean Validation Loss: 0.0663\n",
      "Training with LR=0.001, Batch Size=32, Epochs=300\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0309\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.2634\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0085\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.1745\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0535\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0177\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0492\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.2438\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.1317\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0700\n",
      "Mean Validation Loss: 0.1043\n",
      "Training with LR=0.001, Batch Size=32, Epochs=500\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0360\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0549\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0166\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0244\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.3928\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0385\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0285\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0752\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0319\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1126\n",
      "Mean Validation Loss: 0.0811\n",
      "Training with LR=0.001, Batch Size=32, Epochs=1000\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0410\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0430\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0413\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0322\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1476\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.2591\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0296\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0553\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0975\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0705\n",
      "Mean Validation Loss: 0.0817\n",
      "Training with LR=0.01, Batch Size=8, Epochs=200\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0371\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0539\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0573\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0202\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0431\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0156\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0331\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.5914\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0486\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1671\n",
      "Mean Validation Loss: 0.1068\n",
      "Training with LR=0.01, Batch Size=8, Epochs=300\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0362\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0818\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0250\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.1128\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1452\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0243\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0211\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0826\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0793\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0693\n",
      "Mean Validation Loss: 0.0678\n",
      "Training with LR=0.01, Batch Size=8, Epochs=500\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.1177\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0456\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0405\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0246\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0565\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0220\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0890\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0817\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0225\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1016\n",
      "Mean Validation Loss: 0.0602\n",
      "Training with LR=0.01, Batch Size=8, Epochs=1000\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0329\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0506\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0959\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0469\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1440\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0118\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0171\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0619\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0940\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1400\n",
      "Mean Validation Loss: 0.0695\n",
      "Training with LR=0.01, Batch Size=16, Epochs=200\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0882\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0506\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0238\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0332\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1442\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0943\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0588\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0836\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0361\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0873\n",
      "Mean Validation Loss: 0.0700\n",
      "Training with LR=0.01, Batch Size=16, Epochs=300\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0490\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0772\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0081\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0154\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0882\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0386\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0415\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0462\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0808\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1313\n",
      "Mean Validation Loss: 0.0576\n",
      "Training with LR=0.01, Batch Size=16, Epochs=500\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0401\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0317\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0365\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0747\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0191\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0301\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0226\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0558\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0322\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1510\n",
      "Mean Validation Loss: 0.0494\n",
      "Training with LR=0.01, Batch Size=16, Epochs=1000\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0879\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0278\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0208\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0143\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0728\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0952\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0232\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0674\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0395\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1559\n",
      "Mean Validation Loss: 0.0605\n",
      "Training with LR=0.01, Batch Size=32, Epochs=200\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0863\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0449\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0204\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0341\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1555\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0322\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0772\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0727\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0996\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1485\n",
      "Mean Validation Loss: 0.0771\n",
      "Training with LR=0.01, Batch Size=32, Epochs=300\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0425\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0744\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0198\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0140\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1006\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.0313\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0302\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0613\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0580\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1023\n",
      "Mean Validation Loss: 0.0534\n",
      "Training with LR=0.01, Batch Size=32, Epochs=500\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0537\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0288\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0189\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0148\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.1058\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.1413\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0242\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.1170\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.1192\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.1044\n",
      "Mean Validation Loss: 0.0728\n",
      "Training with LR=0.01, Batch Size=32, Epochs=1000\n",
      "--- Fold 1/10 ---\n",
      "Fold 1, Validation Loss: 0.0429\n",
      "--- Fold 2/10 ---\n",
      "Fold 2, Validation Loss: 0.0369\n",
      "--- Fold 3/10 ---\n",
      "Fold 3, Validation Loss: 0.0257\n",
      "--- Fold 4/10 ---\n",
      "Fold 4, Validation Loss: 0.0991\n",
      "--- Fold 5/10 ---\n",
      "Fold 5, Validation Loss: 0.0285\n",
      "--- Fold 6/10 ---\n",
      "Fold 6, Validation Loss: 0.1489\n",
      "--- Fold 7/10 ---\n",
      "Fold 7, Validation Loss: 0.0266\n",
      "--- Fold 8/10 ---\n",
      "Fold 8, Validation Loss: 0.0463\n",
      "--- Fold 9/10 ---\n",
      "Fold 9, Validation Loss: 0.0368\n",
      "--- Fold 10/10 ---\n",
      "Fold 10, Validation Loss: 0.0860\n",
      "Mean Validation Loss: 0.0577\n",
      "\n",
      "Best Hyperparameters:\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 8\n",
      "Epochs: 200\n",
      "Best Validation Loss: 0.0480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\nLearning Rate: 0.0001\\nBatch Size: 16\\nEpochs: 200\\nBest Validation Loss: 0.0319\\n\\nSecond:\\n\\nThird best:\\nLearning Rate: 0.01\\nBatch Size: 32\\nEpochs: 300\\nBest Validation Loss: 0.0410\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fitness import FitnessFunction\n",
    "from itertools import product\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [8, 16, 32]\n",
    "epochs = [200, 300, 500]\n",
    "\n",
    "# Variables to store the best combination and the lowest loss\n",
    "best_hyperparams = None\n",
    "lowest_loss = float('inf')\n",
    "\n",
    "# Grid search combinations\n",
    "for lr, bs, ep in product(learning_rates, batch_sizes, epochs):\n",
    "    print(f\"Training with LR={lr}, Batch Size={bs}, Epochs={ep}\")\n",
    "    fit_func = FitnessFunction()\n",
    "    fit_func.train_with_cross_validation(flattened_genotype, flattened_fitness, is_classification=False)\n",
    "\n",
    "    # Check if this combination produces the best loss\n",
    "    if fit_func.best_mean_val_loss < lowest_loss:\n",
    "        lowest_loss = fit_func.best_mean_val_loss\n",
    "        best_hyperparams = {'learning_rate': lr, 'batch_size': bs, 'epochs': ep}\n",
    "\n",
    "# Print the best hyperparameters if found\n",
    "if best_hyperparams:\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    print(f\"Learning Rate: {best_hyperparams['learning_rate']}\")\n",
    "    print(f\"Batch Size: {best_hyperparams['batch_size']}\")\n",
    "    print(f\"Epochs: {best_hyperparams['epochs']}\")\n",
    "    print(f\"Best Validation Loss: {lowest_loss:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid hyperparameter combination found.\")\n",
    "\n",
    "# Best Result\n",
    "\"\"\"\"\n",
    "Learning Rate: 0.0001\n",
    "Batch Size: 16\n",
    "Epochs: 200\n",
    "Best Validation Loss: 0.0319\n",
    "\n",
    "Second:\n",
    "Learning Rate: 0.0001\n",
    "Batch Size: 32\n",
    "Epochs: 500\n",
    "Best Validation Loss: 0.0409\n",
    "\n",
    "Third best:\n",
    "Learning Rate: 0.01\n",
    "Batch Size: 32\n",
    "Epochs: 300\n",
    "Best Validation Loss: 0.0410\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
>>>>>>> 56eb08bdfa94219fca82f1a3f16bdffa2933179d
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = pd.read_csv(\"~/class/f24/cbm/evo-life/data/rais_anonymized/csv_rais_anonymized/daily_fitbit_sema_df_unprocessed.csv\")\n",
    "hourly = pd.read_csv(\"~/class/f24/cbm/evo-life/data/rais_anonymized/csv_rais_anonymized/hourly_fitbit_sema_df_unprocessed.csv\")\n",
    "breq = pd.read_csv(\"~/class/f24/cbm/evo-life/data/rais_anonymized/scored_surveys/breq.csv\")\n",
    "panas = pd.read_csv(\"~/class/f24/cbm/evo-life/data/rais_anonymized/scored_surveys/panas.csv\")\n",
    "personality = pd.read_csv(\"~/class/f24/cbm/evo-life/data/rais_anonymized/scored_surveys/personality.csv\")\n",
    "stai = pd.read_csv(\"~/class/f24/cbm/evo-life/data/rais_anonymized/scored_surveys/stai.csv\")\n",
    "ttm = pd.read_csv(\"~/class/f24/cbm/evo-life/data/rais_anonymized/scored_surveys/ttm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the daily data\n",
    "daily.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "# all values will be associated with the ID, let's count how many there are for each\n",
    "num_ids = daily['id'].nunique() # 71\n",
    "\n",
    "# is there an even distribution of dates for the number of records we have (does each ID have the same set of dates?)\n",
    "# subset = daily.loc[daily['id'].isin([\"621e2e8e67b776a24055b564\"])]\n",
    "subset = daily.groupby(\"id\").nunique()\n",
    "subset = subset.reset_index()\n",
    "date_per_id_counts = subset[['id', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_values = daily.isna()\n",
    "nan_values = nan_values.reset_index()\n",
    "true_nan_values = nan_values.sum()\n",
    "true_nan_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(hourly['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly = hourly.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = [\"id\", \"date\", \"hour\",\"distance\", \"steps\", \"calories\", \"age\", \"bpm\", \"bmi\", \"gender\",\n",
    "             \"mindfulness_session\", \"SAD\", \"TIRED\", \"TENSE/ANXIOUS\", \"ENTERTAINMENT\", \"GYM\", \"HOME\",\n",
    "             \"OUTDOORS\"]\n",
    "numeric_coerce = [\"distance\", \"steps\", \"calories\",\"mindfulness_session\", \"SAD\", \"TIRED\", \"TENSE/ANXIOUS\", \"ENTERTAINMENT\", \"GYM\", \"HOME\",\n",
    "             \"OUTDOORS\"]\n",
    "tmp = hourly[all_cols]\n",
    "tmp[numeric_coerce] = tmp[numeric_coerce].apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(np.int64)\n",
    "# first 35 ids are good, fails on 36 and after that for some reason\n",
    "\n",
    "# test_id_subset = ids[36]\n",
    "# tmp = tmp.loc[hourly['id'] == test_id_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_up = tmp.groupby([\"id\", \"date\"]).agg({\n",
    "    \"distance\": ['sum'],\n",
    "    \"steps\": ['sum'],\n",
    "    \"calories\": ['sum'],\n",
    "    \"gender\": ['first'],\n",
    "    'age': ['first'],\n",
    "    # 'bmi': ['max'],\n",
    "    \"mindfulness_session\": ['max'],\n",
    "    \"SAD\": ['max'],\n",
    "    \"TIRED\": [\"max\"],\n",
    "    \"TENSE/ANXIOUS\": ['max'],\n",
    "    \"ENTERTAINMENT\": ['max'],\n",
    "    \"GYM\": [\"max\"],\n",
    "    \"HOME\": ['max'],\n",
    "    \"OUTDOORS\": ['max']\n",
    "    })\n",
    "roll_up.reset_index()\n",
    "roll_up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = daily.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to join the aggregated hourly data to the daily data here to get a full dataset for us to use. \n",
    "to_use = ['id', 'date', 'nremhr', 'rmssd', 'spo2', 'stress_score', 'sleep_points_percentage',\n",
    "          'exertion_points_percentage', 'responsiveness_points_percentage', 'distance', 'activityType',\n",
    "          'bpm', 'lightly_active_minutes', 'moderately_active_minutes', 'very_active_minutes', 'sedentary_minutes',\n",
    "          'mindfulness_session', 'sleep_duration', 'minutesAsleep', 'minutesAwake', 'sleep_efficiency', 'gender',\n",
    "          'bmi', 'TENSE/ANXIOUS', 'TIRED', \"GYM\", \"HOME\", \"OUTDOORS\"]\n",
    "d = daily[to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_values_impute = [\"nremhr\", \"rmssd\", \"spo2\", \"stress_score\", \"sleep_points_percentage\", \n",
    "                        \"exertion_points_percentage\", \"responsiveness_points_percentage\", \"distance\", \n",
    "                        \"minutesAsleep\", \"minutesAwake\", \"sleep_efficiency\", \"bpm\", \n",
    "                        \"lightly_active_minutes\", \"moderately_active_minutes\", \"very_active_minutes\"]\n",
    "d[median_values_impute] = d[median_values_impute].astype(np.float64)\n",
    "\n",
    "median_values = d.groupby('id')[median_values_impute].median()\n",
    "\n",
    "d[median_values_impute] = d[median_values_impute].fillna(median_values)\n",
    "d = d.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates to numbered days\n",
    "d = d.sort_values(by=\"date\")\n",
    "d['day'] = d.groupby(\"id\").cumcount()\n",
    "d.to_csv(\"~/class/f24/cbm/evo-life/prepped/daily.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"~/class/f24/cbm/evo-life/prepped/encodings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../prepped/daily.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_df = gc.df\n",
    "curr_df[curr_df['TENSE/ANXIOUS'] != 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Survey Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how many days each id has\n",
    "days_per_id = d.groupby('id').agg(\n",
    "    {\n",
    "        \"date\": ['count']\n",
    "    }\n",
    ")\n",
    "print(\"Total number of IDs\", days_per_id.__len__())\n",
    "days_per_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates to numbered days\n",
    "d = d.sort_values(by=\"date\")\n",
    "d['day'] = d.groupby(\"id\").cumcount()\n",
    "d.loc[d['id']==\"621e346f67b776a24081744f\"].sort_values(by=\"day\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_records_per_id_breq = breq.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"breq_introjected_regulation\": \"count\"\n",
    "    }\n",
    ")\n",
    "number_records_per_id_breq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breq['user_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in panas:\", panas['user_id'].count())\n",
    "number_records_per_id_panas = panas.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"positive_affect_score\": \"count\"\n",
    "    }\n",
    ")\n",
    "print(\"Number of individual ids represented:\", number_records_per_id_panas.count())\n",
    "number_records_per_id_panas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personality.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in personality:\", personality['user_id'].count())\n",
    "number_records_per_id_personality = personality.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"extraversion\": \"count\"\n",
    "    }\n",
    ")\n",
    "print(\"Number of individual ids represented:\", number_records_per_id_personality.count())\n",
    "number_records_per_id_personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stai.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in stai:\", stai['user_id'].count())\n",
    "number_records_per_id_stai = stai.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"stai_stress\": \"count\"\n",
    "    }\n",
    ")\n",
    "print(\"Number of individual ids represented:\", number_records_per_id_stai.count())\n",
    "number_records_per_id_stai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Number of records in ttm:\", ttm['user_id'].count())\n",
    "number_records_per_id_ttm = ttm.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"ttm_consciousness_raising\": \"count\"\n",
    "    }\n",
    ")\n",
    "print(\"Number of individual ids represented:\", number_records_per_id_ttm.count())\n",
    "number_records_per_id_ttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take subsets of each of the behavioral tables and then join them together so that we have\n",
    "# one table that contains behavioral information on each person who took the surveys\n",
    "\n",
    "# problem: some people have multiple records in the behavior tables. Do I want to average those values together\n",
    "\n",
    "# my decision is to do an average for each value\n",
    "\n",
    "breq_averaged = breq.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"breq_amotivation\":\"mean\",\n",
    "        \"breq_external_regulation\": \"mean\", \n",
    "        \"breq_introjected_regulation\": \"mean\", \n",
    "        \"breq_identified_regulation\": \"mean\", \n",
    "        \"breq_intrinsic_regulation\": \"mean\", \n",
    "        \"breq_self_determination\": \"first\"\n",
    "    }\n",
    ")\n",
    "breq_averaged = breq_averaged.reset_index()\n",
    "print(\"BREQ ID COUNT:\", breq_averaged['user_id'].count())\n",
    "\n",
    "panas_averaged = panas.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"positive_affect_score\":\"mean\",\n",
    "        \"negative_affect_score\": \"mean\"\n",
    "    }\n",
    ")\n",
    "panas_averaged = panas_averaged.reset_index()\n",
    "print(\"PANAS ID COUNT:\", panas_averaged['user_id'].count())\n",
    "\n",
    "personality_averaged = personality.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"agreeableness\": \"mean\",\n",
    "        \"conscientiousness\": \"mean\",\n",
    "        \"stability\": \"mean\", \n",
    "        \"intellect\": \"mean\"\n",
    "    }\n",
    ")\n",
    "\n",
    "personality_averaged = personality_averaged.reset_index()\n",
    "print(\"PERSONALITY ID COUNT:\", personality_averaged['user_id'].count())\n",
    "\n",
    "stai_averaged = stai.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"stai_stress\": \"mean\",\n",
    "        \"stai_stress_category\" : \"first\"\n",
    "    }\n",
    ")\n",
    "\n",
    "stai_averaged = stai_averaged.reset_index()\n",
    "print(\"STAI ID COUNT:\", stai_averaged['user_id'].count())\n",
    "\n",
    "ttm_averaged = ttm.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"ttm_consciousness_raising\": \"mean\", \n",
    "        \"ttm_dramatic_relief\": \"mean\", \n",
    "        \"ttm_environmental_reevaluation\": \"mean\", \n",
    "        \"ttm_self_reevaluation\": \"mean\", \n",
    "        \"ttm_social_liberation\": \"mean\", \n",
    "        \"ttm_counterconditioning\": \"mean\", \n",
    "        \"ttm_helping_relationships\": \"mean\",\n",
    "        \"ttm_reinforcement_management\": \"mean\",\n",
    "        \"ttm_self_liberation\": \"mean\", \n",
    "        \"ttm_stimulus_control\": \"mean\"\n",
    "    }\n",
    ")\n",
    "\n",
    "ttm_averaged = ttm_averaged.reset_index()\n",
    "print(\"TTM ID COUNT:\", ttm_averaged['user_id'].count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_one = breq_averaged.merge(panas_averaged, on='user_id', how='outer')\n",
    "step_two = step_one.merge(personality_averaged, on='user_id', how='outer')\n",
    "step_three = step_two.merge(stai_averaged, on='user_id', how='outer')\n",
    "individuals = step_three.merge(ttm_averaged, on='user_id', how='outer')\n",
    "individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals[individuals.select_dtypes(include=['float']).columns] = individuals.select_dtypes(include=['float']).fillna(0)\n",
    "individuals[individuals.select_dtypes(include=['object']).columns] = individuals.select_dtypes(include=['object']).fillna('undefined')\n",
    "individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals.to_csv(\"~/class/f24/cbm/evo-life/data/prepped/individuals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sort_values(by=['id', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to need to filter down the dataframe to the participants that actually had good data\n",
    "tmp = daily.loc[daily['nightly_temperature'] > 0 ]\n",
    "tmp['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.apply(lambda row: (row == 0).sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import GenomeCreator\n",
    "\n",
    "gc = GenomeCreator()\n",
    "path_to_daily_csv = \"~/class/f24/cbm/evo-life/prepped/daily.csv\"\n",
    "gc.load_data_from_csv(path = path_to_daily_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.create_genome_for_individual(id=\"621e32e667b776a2406d2f1c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_string = \"\"\n",
    "i = 0\n",
    "for c in list(d.columns):\n",
    "    # if i % 5 == 0:\n",
    "    #     final_string += '\\n'\n",
    "    # i += 1\n",
    "    final_string += c + \" \"\n",
    "final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.5"
=======
   "version": "3.12.8"
>>>>>>> 56eb08bdfa94219fca82f1a3f16bdffa2933179d
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
